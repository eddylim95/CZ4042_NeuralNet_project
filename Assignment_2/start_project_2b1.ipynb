{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      }
    },
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "start_project_2b1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XryL94F70BET",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To add a new cell, type '# %%'\n",
        "# To add a new markdown cell, type '# %% [markdown]'\n",
        "# Change working directory from the workspace root to the ipynb file location. Turn this addition off with the DataScience.changeDirOnImportExport setting\n",
        "# ms-python.python added\n",
        "import os\n",
        "try:\n",
        "\tos.chdir(os.path.join(os.getcwd(), 'Assignment_2'))\n",
        "\tprint(os.getcwd())\n",
        "except:\n",
        "\tpass\n",
        "\n",
        "import numpy as np\n",
        "import pandas\n",
        "import tensorflow as tf\n",
        "import csv\n",
        "\n",
        "MAX_DOCUMENT_LENGTH = 100\n",
        "N_FILTERS = 10\n",
        "FILTER_SHAPE1 = [20, 256]\n",
        "POOLING_WINDOW = 4\n",
        "POOLING_STRIDE = 2\n",
        "MAX_LABEL = 15\n",
        "\n",
        "no_epochs = 100\n",
        "lr = 0.01\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "seed = 10\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "def char_cnn_model(x):\n",
        "  \n",
        "  input_layer = tf.reshape(\n",
        "      tf.one_hot(x, 256), [-1, MAX_DOCUMENT_LENGTH, 256, 1])\n",
        "\n",
        "  with tf.variable_scope('CNN_Layer1'):\n",
        "    conv1 = tf.layers.conv2d(\n",
        "        input_layer,\n",
        "        filters=N_FILTERS,\n",
        "        kernel_size=FILTER_SHAPE1,\n",
        "        padding='VALID',\n",
        "        activation=tf.nn.relu)\n",
        "    pool1 = tf.layers.max_pooling2d(\n",
        "        conv1,\n",
        "        pool_size=POOLING_WINDOW,\n",
        "        strides=POOLING_STRIDE,\n",
        "        padding='SAME')\n",
        "\n",
        "    pool1 = tf.squeeze(tf.reduce_max(pool1, 1), squeeze_dims=[1])\n",
        "\n",
        "  logits = tf.layers.dense(pool1, MAX_LABEL, activation=None)\n",
        "\n",
        "  return input_layer, logits\n",
        "\n",
        "\n",
        "def read_data_chars():\n",
        "  \n",
        "  x_train, y_train, x_test, y_test = [], [], [], []\n",
        "\n",
        "  with open('train_medium.csv', encoding='utf-8') as filex:\n",
        "    reader = csv.reader(filex)\n",
        "    for row in reader:\n",
        "      x_train.append(row[1])\n",
        "      y_train.append(int(row[0]))\n",
        "\n",
        "  with open('test_medium.csv', encoding='utf-8') as filex:\n",
        "    reader = csv.reader(filex)\n",
        "    for row in reader:\n",
        "      x_test.append(row[1])\n",
        "      y_test.append(int(row[0]))\n",
        "  \n",
        "  x_train = pandas.Series(x_train)\n",
        "  y_train = pandas.Series(y_train)\n",
        "  x_test = pandas.Series(x_test)\n",
        "  y_test = pandas.Series(y_test)\n",
        "  \n",
        "  \n",
        "  char_processor = tf.contrib.learn.preprocessing.ByteProcessor(MAX_DOCUMENT_LENGTH)\n",
        "  x_train = np.array(list(char_processor.fit_transform(x_train)))\n",
        "  x_test = np.array(list(char_processor.transform(x_test)))\n",
        "  y_train = y_train.values\n",
        "  y_test = y_test.values\n",
        "  \n",
        "  return x_train, y_train, x_test, y_test\n",
        "\n",
        "  \n",
        "def main():\n",
        "  \n",
        "  x_train, y_train, x_test, y_test = read_data_chars()\n",
        "\n",
        "  print(len(x_train))\n",
        "  print(len(x_test))\n",
        "\n",
        "  # Create the model\n",
        "  x = tf.placeholder(tf.int64, [None, MAX_DOCUMENT_LENGTH])\n",
        "  y_ = tf.placeholder(tf.int64)\n",
        "\n",
        "  inputs, logits = char_cnn_model(x)\n",
        "\n",
        "  # Optimizer\n",
        "  entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(y_, MAX_LABEL), logits=logits))\n",
        "  train_op = tf.train.AdamOptimizer(lr).minimize(entropy)\n",
        "\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  # training\n",
        "  loss = []\n",
        "  for e in range(no_epochs):\n",
        "    _, loss_  = sess.run([train_op, entropy], {x: x_train, y_: y_train})\n",
        "    loss.append(loss_)\n",
        "\n",
        "\n",
        "    if e%1 == 0:\n",
        "      print('iter: %d, entropy: %g'%(e, loss[e]))\n",
        "  \n",
        "  sess.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}